package com.hankcs.hanlp.classification.classifiers;

import com.hankcs.hanlp.utility.MathUtility;
import com.hankcs.hanlp.collection.trie.bintrie.BinTrie;
import com.hankcs.hanlp.classification.corpus.*;
import com.hankcs.hanlp.classification.features.ChiSquareFeatureExtractor;
import com.hankcs.hanlp.classification.features.BaseFeatureData;
import com.hankcs.hanlp.classification.models.AbstractModel;
import com.hankcs.hanlp.classification.models.NaiveBayesModel;

import static com.hankcs.hanlp.classification.utilities.io.ConsoleLogger.logger;

import java.util.*;

/**
 * å®žçŽ°ä¸€ä¸ªåŸºäºŽå¤šé¡¹å¼?è´?å?¶æ–¯æ¨¡åž‹çš„æ–‡æœ¬åˆ†ç±»å™¨
 */
public class NaiveBayesClassifier extends AbstractClassifier
{

    private NaiveBayesModel model;

    /**
     * ç”±è®­ç»ƒç»“æžœæž„é€ ä¸€ä¸ªè´?å?¶æ–¯åˆ†ç±»å™¨ï¼Œé€šå¸¸ç”¨æ?¥åŠ è½½ç£?ç›˜ä¸­çš„åˆ†ç±»å™¨
     *
     * @param naiveBayesModel
     */
    public NaiveBayesClassifier(NaiveBayesModel naiveBayesModel)
    {
        this.model = naiveBayesModel;
    }

    /**
     * æž„é€ ä¸€ä¸ªç©ºç™½çš„è´?å?¶æ–¯åˆ†ç±»å™¨ï¼Œé€šå¸¸å‡†å¤‡ç”¨æ?¥è¿›è¡Œè®­ç»ƒ
     */
    public NaiveBayesClassifier()
    {
        this(null);
    }

    /**
     * èŽ·å?–è®­ç»ƒç»“æžœ
     *
     * @return
     */
    public NaiveBayesModel getNaiveBayesModel()
    {
        return model;
    }

    public void train(IDataSet dataSet)
    {
        logger.out("åŽŸå§‹æ•°æ?®é›†å¤§å°?:%d\n", dataSet.size());
        //é€‰æ‹©æœ€ä½³ç‰¹å¾?
        BaseFeatureData featureData = selectFeatures(dataSet);

        //åˆ?å§‹åŒ–åˆ†ç±»å™¨æ‰€ç”¨çš„æ•°æ?®
        model = new NaiveBayesModel();
        model.n = featureData.n; //æ ·æœ¬æ•°é‡?
        model.d = featureData.featureCategoryJointCount.length; //ç‰¹å¾?æ•°é‡?

        model.c = featureData.categoryCounts.length; //ç±»ç›®æ•°é‡?
        model.logPriors = new TreeMap<Integer, Double>();

        int sumCategory;
        for (int category = 0; category < featureData.categoryCounts.length; category++)
        {
            sumCategory = featureData.categoryCounts[category];
            model.logPriors.put(category, Math.log((double) sumCategory / model.n));
        }

        //æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å¤„ç?†ï¼ˆå?ˆç§°åŠ ä¸€å¹³æ»‘ï¼‰ï¼Œè¿™æ—¶éœ€è¦?ä¼°è®¡æ¯?ä¸ªç±»ç›®ä¸‹çš„å®žä¾‹
        Map<Integer, Double> featureOccurrencesInCategory = new TreeMap<Integer, Double>();

        Double featureOccSum;
        for (Integer category : model.logPriors.keySet())
        {
            featureOccSum = 0.0;
            for (int feature = 0; feature < featureData.featureCategoryJointCount.length; feature++)
            {

                featureOccSum += featureData.featureCategoryJointCount[feature][category];
            }
            featureOccurrencesInCategory.put(category, featureOccSum);
        }

        //å¯¹æ•°ä¼¼ç„¶ä¼°è®¡
        int count;
        int[] featureCategoryCounts;
        double logLikelihood;
        for (Integer category : model.logPriors.keySet())
        {
            for (int feature = 0; feature < featureData.featureCategoryJointCount.length; feature++)
            {

                featureCategoryCounts = featureData.featureCategoryJointCount[feature];

                count = featureCategoryCounts[category];

                logLikelihood = Math.log((count + 1.0) / (featureOccurrencesInCategory.get(category) + model.d));
                if (!model.logLikelihoods.containsKey(feature))
                {
                    model.logLikelihoods.put(feature, new TreeMap<Integer, Double>());
                }
                model.logLikelihoods.get(feature).put(category, logLikelihood);
            }
        }
        logger.out("è´?å?¶æ–¯ç»Ÿè®¡ç»“æ?Ÿ\n");
        model.catalog = dataSet.getCatalog().toArray();
        model.tokenizer = dataSet.getTokenizer();
        model.wordIdTrie = featureData.wordIdTrie;
    }

    public AbstractModel getModel()
    {
        return model;
    }

    public Map<String, Double> predict(String text) throws IllegalArgumentException, IllegalStateException
    {
        if (model == null)
        {
            throw new IllegalStateException("æœªè®­ç»ƒæ¨¡åž‹ï¼?æ— æ³•æ‰§è¡Œé¢„æµ‹ï¼?");
        }
        if (text == null)
        {
            throw new IllegalArgumentException("å?‚æ•° text == null");
        }

        //åˆ†è¯?ï¼Œåˆ›å»ºæ–‡æ¡£
        Document doc = new Document(model.wordIdTrie, model.tokenizer.segment(text));

        return predict(doc);
    }

    @Override
    public double[] categorize(Document document) throws IllegalArgumentException, IllegalStateException
    {
        Integer category;
        Integer feature;
        Integer occurrences;
        Double logprob;

        double[] predictionScores = new double[model.catalog.length];
        for (Map.Entry<Integer, Double> entry1 : model.logPriors.entrySet())
        {
            category = entry1.getKey();
            logprob = entry1.getValue(); //ç”¨ç±»ç›®çš„å¯¹æ•°ä¼¼ç„¶åˆ?å§‹åŒ–æ¦‚çŽ‡

            //å¯¹æ–‡æ¡£ä¸­çš„æ¯?ä¸ªç‰¹å¾?
            for (Map.Entry<Integer, int[]> entry2 : document.tfMap.entrySet())
            {
                feature = entry2.getKey();

                if (!model.logLikelihoods.containsKey(feature))
                {
                    continue; //å¦‚æžœåœ¨æ¨¡åž‹ä¸­æ‰¾ä¸?åˆ°å°±è·³è¿‡äº†
                }

                occurrences = entry2.getValue()[0]; //èŽ·å?–å…¶åœ¨æ–‡æ¡£ä¸­çš„é¢‘æ¬¡

                logprob += occurrences * model.logLikelihoods.get(feature).get(category); //å°†å¯¹æ•°ä¼¼ç„¶ä¹˜ä¸Šé¢‘æ¬¡
            }
            predictionScores[category] = logprob;
        }

        if (configProbabilityEnabled) MathUtility.normalizeExp(predictionScores);
        return predictionScores;
    }

    /**
     * ç»Ÿè®¡ç‰¹å¾?å¹¶ä¸”æ‰§è¡Œç‰¹å¾?é€‰æ‹©ï¼Œè¿”å›žä¸€ä¸ªFeatureStatså¯¹è±¡ï¼Œç”¨äºŽè®¡ç®—æ¨¡åž‹ä¸­çš„æ¦‚çŽ‡
     *
     * @param dataSet
     * @return
     */
    protected BaseFeatureData selectFeatures(IDataSet dataSet)
    {
        ChiSquareFeatureExtractor chiSquareFeatureExtractor = new ChiSquareFeatureExtractor();

        logger.start("ä½¿ç”¨å?¡æ–¹æ£€æµ‹é€‰æ‹©ç‰¹å¾?ä¸­...");
        //FeatureStatså¯¹è±¡åŒ…å?«æ–‡æ¡£ä¸­æ‰€æœ‰ç‰¹å¾?å?Šå…¶ç»Ÿè®¡ä¿¡æ?¯
        BaseFeatureData featureData = chiSquareFeatureExtractor.extractBasicFeatureData(dataSet); //æ‰§è¡Œç»Ÿè®¡

        //æˆ‘ä»¬ä¼ å…¥è¿™äº›ç»Ÿè®¡ä¿¡æ?¯åˆ°ç‰¹å¾?é€‰æ‹©ç®—æ³•ä¸­ï¼Œå¾—åˆ°ç‰¹å¾?ä¸Žå…¶åˆ†å€¼
        Map<Integer, Double> selectedFeatures = chiSquareFeatureExtractor.chi_square(featureData);

        //ä»Žç»Ÿè®¡æ•°æ?®ä¸­åˆ æŽ‰æ— ç”¨çš„ç‰¹å¾?å¹¶é‡?å»ºç‰¹å¾?æ˜ å°„è¡¨
        int[][] featureCategoryJointCount = new int[selectedFeatures.size()][];
        featureData.wordIdTrie = new BinTrie<Integer>();
        String[] wordIdArray = dataSet.getLexicon().getWordIdArray();
        int p = -1;
        for (Integer feature : selectedFeatures.keySet())
        {
            featureCategoryJointCount[++p] = featureData.featureCategoryJointCount[feature];
            featureData.wordIdTrie.put(wordIdArray[feature], p);
        }
        logger.finish(",é€‰ä¸­ç‰¹å¾?æ•°:%d / %d = %.2f%%\n", featureCategoryJointCount.length,
                      featureData.featureCategoryJointCount.length,
                      featureCategoryJointCount.length / (double)featureData.featureCategoryJointCount.length * 100.);
        featureData.featureCategoryJointCount = featureCategoryJointCount;

        return featureData;
    }
}
