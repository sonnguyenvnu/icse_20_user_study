package com.hankcs.hanlp.mining.word;

import com.hankcs.hanlp.algorithm.MaxHeap;
import com.hankcs.hanlp.utility.LexiconUtility;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.StringReader;
import java.util.*;
import java.util.regex.Pattern;

/**
 * æ–°è¯?å?‘çŽ°å·¥å…·<br>
 * åœ¨å®žçŽ°ä¸Šå?‚è€ƒäº†ï¼šhttps://github.com/Moonshile/ChineseWordSegmentation
 *
 * @author hankcs
 */
public class NewWordDiscover
{
    private int max_word_len;
    private float min_freq;
    private float min_entropy;
    private float min_aggregation;
    private boolean filter;

    public NewWordDiscover()
    {
        this(4, 0.00005f, .4f, 1.2f, false);
    }

    /**
     * æž„é€ ä¸€ä¸ªæ–°è¯?è¯†åˆ«å·¥å…·
     *
     * @param max_word_len    è¯?è¯­æœ€é•¿é•¿åº¦
     * @param min_freq        è¯?è¯­æœ€ä½Žé¢‘çŽ‡
     * @param min_entropy     è¯?è¯­æœ€ä½Žç†µ
     * @param min_aggregation è¯?è¯­æœ€ä½Žäº’ä¿¡æ?¯
     * @param filter          æ˜¯å?¦è¿‡æ»¤æŽ‰HanLPä¸­çš„è¯?åº“ä¸­å·²å­˜åœ¨çš„è¯?è¯­
     */
    public NewWordDiscover(int max_word_len, float min_freq, float min_entropy, float min_aggregation, boolean filter)
    {
        this.max_word_len = max_word_len;
        this.min_freq = min_freq;
        this.min_entropy = min_entropy;
        this.min_aggregation = min_aggregation;
        this.filter = filter;
    }

    /**
     * æ??å?–è¯?è¯­
     *
     * @param reader å¤§æ–‡æœ¬
     * @param size   éœ€è¦?æ??å?–è¯?è¯­çš„æ•°é‡?
     * @return ä¸€ä¸ªè¯?è¯­åˆ—è¡¨
     */
    public List<WordInfo> discover(BufferedReader reader, int size) throws IOException
    {
        String doc;
        Map<String, WordInfo> word_cands = new TreeMap<String, WordInfo>();
        int totalLength = 0;
        Pattern delimiter = Pattern.compile("[\\s\\d,.<>/?:;'\"\\[\\]{}()\\|~!@#$%^&*\\-_=+ï¼Œã€‚ã€Šã€‹ã€?ï¼Ÿï¼šï¼›â€œâ€?â€˜â€™ï½›ï½?ã€?ã€‘ï¼ˆï¼‰â€¦ï¿¥ï¼?â€”â”„ï¼?]+");
        while ((doc = reader.readLine()) != null)
        {
            doc = delimiter.matcher(doc).replaceAll("\0");
            int docLength = doc.length();
            for (int i = 0; i < docLength; ++i)
            {
                int end = Math.min(i + 1 + max_word_len, docLength + 1);
                for (int j = i + 1; j < end; ++j)
                {
                    String word = doc.substring(i, j);
                    if (word.indexOf('\0') >= 0)
                        continue; // å?«æœ‰åˆ†éš”ç¬¦çš„ä¸?è®¤ä¸ºæ˜¯è¯?è¯­
                    WordInfo info = word_cands.get(word);
                    if (info == null)
                    {
                        info = new WordInfo(word);
                        word_cands.put(word, info);
                    }
                    info.update(i == 0 ? '\0' : doc.charAt(i - 1), j < docLength ? doc.charAt(j) : '\0');
                }
            }
            totalLength += docLength;
        }

        for (WordInfo info : word_cands.values())
        {
            info.computeProbabilityEntropy(totalLength);
        }
        for (WordInfo info : word_cands.values())
        {
            info.computeAggregation(word_cands);
        }
        // è¿‡æ»¤
        List<WordInfo> wordInfoList = new LinkedList<WordInfo>(word_cands.values());
        ListIterator<WordInfo> listIterator = wordInfoList.listIterator();
        while (listIterator.hasNext())
        {
            WordInfo info = listIterator.next();
            if (info.text.trim().length() < 2 || info.p < min_freq || info.entropy < min_entropy || info.aggregation < min_aggregation
                || (filter && LexiconUtility.getFrequency(info.text) > 0)
                )
            {
                listIterator.remove();
            }
        }
        // æŒ‰ç…§é¢‘çŽ‡æŽ’åº?
        MaxHeap<WordInfo> topN = new MaxHeap<WordInfo>(size, new Comparator<WordInfo>()
        {
            public int compare(WordInfo o1, WordInfo o2)
            {
                return Float.compare(o1.p, o2.p);
            }
        });
        topN.addAll(wordInfoList);

        return topN.toList();
    }

    /**
     * æ??å?–è¯?è¯­
     *
     * @param doc  å¤§æ–‡æœ¬
     * @param size éœ€è¦?æ??å?–è¯?è¯­çš„æ•°é‡?
     * @return ä¸€ä¸ªè¯?è¯­åˆ—è¡¨
     */
    public List<WordInfo> discover(String doc, int size)
    {
        try
        {
            return discover(new BufferedReader(new StringReader(doc)), size);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }
}
